{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Prometheus Relation Model\n",
    "\n",
    "`sbt publishM2`\n",
    "\n",
    "Repeat if the project has been updated, and also do:\n",
    "\n",
    "`rm -rf ~/.m2/repository/sonymobile`\n",
    "\n",
    "`rm -rf metastore_db`\n",
    "\n",
    "(*Important*: `kernel -> restart all` otherwise the previously used version of the prometheus-relation-model lib will be used again! Very confusing!) <- not sure if this is correct\n",
    "\n",
    "`prometheus-relation-model` requires [docforia](https://github.com/marcusklang/docforia), which must be git-cloned and `mvn install`:ed first (not yet on Maven repository unfortunately)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Marking sonymobile:prometheus-relation-model_2.10:0.0.1-SNAPSHOT for download\n",
      "Preparing to fetch from:\n",
      "-> file:/var/folders/28/5mj8jbrd13z_nssxk35nd25c0000gn/T/toree_add_deps8523781516040407919/\n",
      "-> file:/Users/axel/.m2/repository\n",
      "-> https://repo1.maven.org/maven2\n",
      "-> New file at /Users/axel/.m2/repository/se/lth/cs/nlp/docforia/1.0-SNAPSHOT/docforia-1.0-SNAPSHOT.jar\n",
      "-> New file at /Users/axel/.m2/repository/com/google/protobuf/protobuf-java/2.6.1/protobuf-java-2.6.1.jar\n",
      "-> New file at /Users/axel/.m2/repository/org/hamcrest/hamcrest-core/1.3/hamcrest-core-1.3.jar\n",
      "-> New file at /Users/axel/.m2/repository/junit/junit/4.11/junit-4.11.jar\n",
      "-> New file at /Users/axel/.m2/repository/org/xerial/snappy/snappy-java/1.1.2.1/snappy-java-1.1.2.1.jar\n",
      "-> New file at /Users/axel/.m2/repository/sonymobile/prometheus-relation-model_2.10/0.0.1-SNAPSHOT/prometheus-relation-model_2.10-0.0.1-SNAPSHOT.jar\n",
      "-> New file at /Users/axel/.m2/repository/it/unimi/dsi/fastutil/6.3/fastutil-6.3.jar\n",
      "-> New file at /var/folders/28/5mj8jbrd13z_nssxk35nd25c0000gn/T/toree_add_deps8523781516040407919/https/repo1.maven.org/maven2/com/fasterxml/jackson/core/jackson-annotations/2.8.6/jackson-annotations-2.8.6.jar\n",
      "-> New file at /var/folders/28/5mj8jbrd13z_nssxk35nd25c0000gn/T/toree_add_deps8523781516040407919/https/repo1.maven.org/maven2/com/thoughtworks/paranamer/paranamer/2.8/paranamer-2.8.jar\n",
      "-> New file at /var/folders/28/5mj8jbrd13z_nssxk35nd25c0000gn/T/toree_add_deps8523781516040407919/https/repo1.maven.org/maven2/com/fasterxml/jackson/module/jackson-module-paranamer/2.8.6/jackson-module-paranamer-2.8.6.jar\n",
      "-> New file at /var/folders/28/5mj8jbrd13z_nssxk35nd25c0000gn/T/toree_add_deps8523781516040407919/https/repo1.maven.org/maven2/com/fasterxml/jackson/core/jackson-core/2.8.6/jackson-core-2.8.6.jar\n",
      "-> New file at /var/folders/28/5mj8jbrd13z_nssxk35nd25c0000gn/T/toree_add_deps8523781516040407919/https/repo1.maven.org/maven2/org/rogach/scallop_2.10/2.1.0/scallop_2.10-2.1.0.jar\n",
      "-> New file at /var/folders/28/5mj8jbrd13z_nssxk35nd25c0000gn/T/toree_add_deps8523781516040407919/https/repo1.maven.org/maven2/com/fasterxml/jackson/module/jackson-module-scala_2.10/2.8.6/jackson-module-scala_2.10-2.8.6.jar\n",
      "-> New file at /var/folders/28/5mj8jbrd13z_nssxk35nd25c0000gn/T/toree_add_deps8523781516040407919/https/repo1.maven.org/maven2/com/fasterxml/jackson/core/jackson-databind/2.8.6/jackson-databind-2.8.6.jar\n"
     ]
    }
   ],
   "source": [
    "%AddDeps sonymobile prometheus-relation-model_2.10 0.0.1-SNAPSHOT --transitive --repository file:/Users/axel/.m2/repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Marking se.lth.cs.nlp:docforia:1.0-SNAPSHOT for download\n",
      "Preparing to fetch from:\n",
      "-> file:/var/folders/28/5mj8jbrd13z_nssxk35nd25c0000gn/T/toree_add_deps3852866345726084937/\n",
      "-> file:/Users/axel/.m2/repository\n",
      "-> https://repo1.maven.org/maven2\n",
      "-> New file at /Users/axel/.m2/repository/se/lth/cs/nlp/docforia/1.0-SNAPSHOT/docforia-1.0-SNAPSHOT.jar\n",
      "-> New file at /Users/axel/.m2/repository/com/fasterxml/jackson/core/jackson-annotations/2.7.0/jackson-annotations-2.7.0.jar\n",
      "-> New file at /Users/axel/.m2/repository/junit/junit/4.11/junit-4.11.jar\n",
      "-> New file at /Users/axel/.m2/repository/org/hamcrest/hamcrest-core/1.3/hamcrest-core-1.3.jar\n",
      "-> New file at /Users/axel/.m2/repository/com/fasterxml/jackson/core/jackson-databind/2.7.3/jackson-databind-2.7.3.jar\n",
      "-> New file at /Users/axel/.m2/repository/com/fasterxml/jackson/core/jackson-core/2.7.3/jackson-core-2.7.3.jar\n",
      "-> New file at /Users/axel/.m2/repository/com/google/protobuf/protobuf-java/2.6.1/protobuf-java-2.6.1.jar\n",
      "-> New file at /Users/axel/.m2/repository/it/unimi/dsi/fastutil/6.3/fastutil-6.3.jar\n"
     ]
    }
   ],
   "source": [
    "%AddDeps se.lth.cs.nlp docforia 1.0-SNAPSHOT --transitive --repository file:/Users/axel/.m2/repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import com.sony.relationmodel.CorpusReader\n",
    "\n",
    "val docs = CorpusReader.readCorpus(\"/Users/axel/utveckling/prometheus/data/wikipedia-corpus-herd/sv\")(sqlContext, sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "val gp = docs.filter(_.uri() == \"urn:wikidata:Q53747\").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Lse.lth.cs.docforia.Document;@4db7f69a\n"
     ]
    }
   ],
   "source": [
    "println(gp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class org.apache.spark.mllib.linalg.SparseVector\n"
     ]
    }
   ],
   "source": [
    "import org.apache.spark.mllib.linalg.{SparseVector, Vector, Vectors}\n",
    "val sparseV = Vectors.sparse(50, Array(0, 2), Array(1.0, 3.0))\n",
    "println(sparseV.getClass)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: Compile Error\n",
       "Message: <console>:34: error: object prometheus is not a member of package com.sony\n",
       "         import com.sony.prometheus.stages.TrainingDataExtractor\n",
       "                         ^\n",
       "StackTrace: "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import com.sony.prometheus.stages.TrainingDataExtractor\n",
    "val relations = RelationsReader.readRelations(\"/Users/axel/utveckling/prometheus/data/0.4.0/entities\")\n",
    "val sentences = TrainingDataExtractor.extract(docs, relations)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# One-vs-All"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import java.util.concurrent.TimeUnit.{NANOSECONDS => NANO}\n",
    "\n",
    "import org.apache.spark.{SparkContext, SparkConf}\n",
    "import org.apache.spark.ml.classification.{OneVsRest, LogisticRegression}\n",
    "import org.apache.spark.ml.util.MetadataUtils\n",
    "import org.apache.spark.mllib.evaluation.MulticlassMetrics\n",
    "import org.apache.spark.mllib.linalg.Vector\n",
    "import org.apache.spark.sql.DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: org.apache.hadoop.mapred.InvalidInputException\n",
       "Message: Input path does not exist: file:/Users/axel/utveckling/prometheus/relation-model/iris.scale\n",
       "StackTrace: org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:285)\n",
       "org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:228)\n",
       "org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:313)\n",
       "org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:202)\n",
       "org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:239)\n",
       "org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:237)\n",
       "scala.Option.getOrElse(Option.scala:120)\n",
       "org.apache.spark.rdd.RDD.partitions(RDD.scala:237)\n",
       "org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:35)\n",
       "org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:239)\n",
       "org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:237)\n",
       "scala.Option.getOrElse(Option.scala:120)\n",
       "org.apache.spark.rdd.RDD.partitions(RDD.scala:237)\n",
       "org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:35)\n",
       "org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:239)\n",
       "org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:237)\n",
       "scala.Option.getOrElse(Option.scala:120)\n",
       "org.apache.spark.rdd.RDD.partitions(RDD.scala:237)\n",
       "org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:35)\n",
       "org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:239)\n",
       "org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:237)\n",
       "scala.Option.getOrElse(Option.scala:120)\n",
       "org.apache.spark.rdd.RDD.partitions(RDD.scala:237)\n",
       "org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:35)\n",
       "org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:239)\n",
       "org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:237)\n",
       "scala.Option.getOrElse(Option.scala:120)\n",
       "org.apache.spark.rdd.RDD.partitions(RDD.scala:237)\n",
       "org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:35)\n",
       "org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:239)\n",
       "org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:237)\n",
       "scala.Option.getOrElse(Option.scala:120)\n",
       "org.apache.spark.rdd.RDD.partitions(RDD.scala:237)\n",
       "org.apache.spark.SparkContext.runJob(SparkContext.scala:1952)\n",
       "org.apache.spark.rdd.RDD$$anonfun$reduce$1.apply(RDD.scala:1025)\n",
       "org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150)\n",
       "org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:111)\n",
       "org.apache.spark.rdd.RDD.withScope(RDD.scala:316)\n",
       "org.apache.spark.rdd.RDD.reduce(RDD.scala:1007)\n",
       "org.apache.spark.mllib.util.MLUtils$.loadLibSVMFile(MLUtils.scala:105)\n",
       "org.apache.spark.mllib.util.MLUtils$.loadLibSVMFile(MLUtils.scala:134)\n",
       "org.apache.spark.ml.source.libsvm.LibSVMRelation.buildScan(LibSVMRelation.scala:49)\n",
       "org.apache.spark.sql.execution.datasources.DataSourceStrategy$.apply(DataSourceStrategy.scala:135)\n",
       "org.apache.spark.sql.catalyst.planning.QueryPlanner$$anonfun$1.apply(QueryPlanner.scala:58)\n",
       "org.apache.spark.sql.catalyst.planning.QueryPlanner$$anonfun$1.apply(QueryPlanner.scala:58)\n",
       "scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)\n",
       "org.apache.spark.sql.catalyst.planning.QueryPlanner.plan(QueryPlanner.scala:59)\n",
       "org.apache.spark.sql.catalyst.planning.QueryPlanner.planLater(QueryPlanner.scala:54)\n",
       "org.apache.spark.sql.execution.SparkStrategies$BasicOperators$.apply(SparkStrategies.scala:334)\n",
       "org.apache.spark.sql.catalyst.planning.QueryPlanner$$anonfun$1.apply(QueryPlanner.scala:58)\n",
       "org.apache.spark.sql.catalyst.planning.QueryPlanner$$anonfun$1.apply(QueryPlanner.scala:58)\n",
       "scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)\n",
       "org.apache.spark.sql.catalyst.planning.QueryPlanner.plan(QueryPlanner.scala:59)\n",
       "org.apache.spark.sql.catalyst.planning.QueryPlanner.planLater(QueryPlanner.scala:54)\n",
       "org.apache.spark.sql.execution.SparkStrategies$BasicOperators$.apply(SparkStrategies.scala:345)\n",
       "org.apache.spark.sql.catalyst.planning.QueryPlanner$$anonfun$1.apply(QueryPlanner.scala:58)\n",
       "org.apache.spark.sql.catalyst.planning.QueryPlanner$$anonfun$1.apply(QueryPlanner.scala:58)\n",
       "scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)\n",
       "org.apache.spark.sql.catalyst.planning.QueryPlanner.plan(QueryPlanner.scala:59)\n",
       "org.apache.spark.sql.execution.QueryExecution.sparkPlan$lzycompute(QueryExecution.scala:47)\n",
       "org.apache.spark.sql.execution.QueryExecution.sparkPlan(QueryExecution.scala:45)\n",
       "org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:52)\n",
       "org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:52)\n",
       "org.apache.spark.sql.execution.CacheManager$$anonfun$cacheQuery$1.apply(CacheManager.scala:97)\n",
       "org.apache.spark.sql.execution.CacheManager.writeLock(CacheManager.scala:60)\n",
       "org.apache.spark.sql.execution.CacheManager.cacheQuery(CacheManager.scala:84)\n",
       "org.apache.spark.sql.DataFrame.persist(DataFrame.scala:1582)\n",
       "org.apache.spark.sql.DataFrame.cache(DataFrame.scala:1591)\n",
       "$line36.$read$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$anonfun$1.apply(<console>:38)\n",
       "$line36.$read$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$anonfun$1.apply(<console>:38)\n",
       "scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\n",
       "scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\n",
       "scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)\n",
       "scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:108)\n",
       "scala.collection.TraversableLike$class.map(TraversableLike.scala:244)\n",
       "scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:108)\n",
       "$line36.$read$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:38)\n",
       "$line36.$read$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:45)\n",
       "$line36.$read$$iwC$$iwC$$iwC$$iwC.<init>(<console>:47)\n",
       "$line36.$read$$iwC$$iwC$$iwC.<init>(<console>:49)\n",
       "$line36.$read$$iwC$$iwC.<init>(<console>:51)\n",
       "$line36.$read$$iwC.<init>(<console>:53)\n",
       "$line36.$read.<init>(<console>:55)\n",
       "$line36.$read$.<init>(<console>:59)\n",
       "$line36.$read$.<clinit>(<console>)\n",
       "$line36.$eval$.<init>(<console>:7)\n",
       "$line36.$eval$.<clinit>(<console>)\n",
       "$line36.$eval.$print(<console>)\n",
       "sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
       "sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
       "sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
       "java.lang.reflect.Method.invoke(Method.java:497)\n",
       "org.apache.spark.repl.SparkIMain$ReadEvalPrint.call(SparkIMain.scala:1065)\n",
       "org.apache.spark.repl.SparkIMain$Request.loadAndRun(SparkIMain.scala:1346)\n",
       "org.apache.spark.repl.SparkIMain.loadAndRunReq$1(SparkIMain.scala:840)\n",
       "org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:871)\n",
       "org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:819)\n",
       "org.apache.toree.kernel.interpreter.scala.ScalaInterpreter$$anonfun$interpretAddTask$1$$anonfun$apply$3.apply(ScalaInterpreter.scala:361)\n",
       "org.apache.toree.kernel.interpreter.scala.ScalaInterpreter$$anonfun$interpretAddTask$1$$anonfun$apply$3.apply(ScalaInterpreter.scala:356)\n",
       "org.apache.toree.global.StreamState$.withStreams(StreamState.scala:81)\n",
       "org.apache.toree.kernel.interpreter.scala.ScalaInterpreter$$anonfun$interpretAddTask$1.apply(ScalaInterpreter.scala:355)\n",
       "org.apache.toree.kernel.interpreter.scala.ScalaInterpreter$$anonfun$interpretAddTask$1.apply(ScalaInterpreter.scala:355)\n",
       "org.apache.toree.utils.TaskManager$$anonfun$add$2$$anon$1.run(TaskManager.scala:140)\n",
       "java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n",
       "java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n",
       "java.lang.Thread.run(Thread.java:745)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val inputData = sqlContext.read.format(\"libsvm\").load(\"iris.scale\")\n",
    "val data = inputData.randomSplit(Array(0.8, 0.2))\n",
    "\n",
    "val Array(train, test) = data.map(_.cache())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- label: double (nullable = false)\n",
      " |-- features: vector (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "val classifier = new LogisticRegression().setMaxIter(10).setTol(1E-6).setFitIntercept(true)\n",
    "inputData.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "oneVsRest_1f546bd6df4f"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val ovr = new OneVsRest()\n",
    "ovr.setClassifier(classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: Compile Error\n",
       "Message: <console>:33: error: not found: value train\n",
       "         val ovrModel = ovr.fit(train)\n",
       "                                ^\n",
       "StackTrace: "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val ovrModel = ovr.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: Compile Error\n",
       "Message: <console>:31: error: not found: value ovrModel\n",
       "         val predictions = ovrModel.transform(test)\n",
       "                           ^\n",
       "StackTrace: "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val predictions = ovrModel.transform(test)\n",
    "val predictionsAndLabels = predictions.select(\"prediction\", \"label\").map(row => (row.getDouble(0), row.getDouble(1)))\n",
    "predictions.show(100)\n",
    "predictions.take(1)(0).get(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: Compile Error\n",
       "Message: <console>:31: error: not found: value predictionsAndLabels\n",
       "         val metrics = new MulticlassMetrics(predictionsAndLabels)\n",
       "                                             ^\n",
       "StackTrace: "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val metrics = new MulticlassMetrics(predictionsAndLabels)\n",
    "val confusionMatrix = metrics.confusionMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: Compile Error\n",
       "Message: <console>:32: error: not found: value confusionMatrix\n",
       "              println(s\" Confusion Matrix\\n ${confusionMatrix.toString}\\n\")\n",
       "                                              ^\n",
       "StackTrace: "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "println(s\" Confusion Matrix\\n ${confusionMatrix.toString}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Corpusreader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import java.io.IOError\n",
    "import org.apache.log4j.LogManager\n",
    "import org.apache.spark.rdd.RDD\n",
    "import org.apache.spark.sql.{DataFrame, SQLContext}\n",
    "import org.apache.spark.{Accumulator, SparkContext}\n",
    "\n",
    "import se.lth.cs.docforia.Document\n",
    "import se.lth.cs.docforia.memstore.MemoryDocumentIO\n",
    "\n",
    "object CorpusReader {\n",
    "\n",
    "  /** Returns an RDD of [[se.lth.cs.docforia.Document]]\n",
    "   *\n",
    "   *  @param file - the path to the corpus\n",
    "   *  @param sampleSize - sample this fraction of the corpus (default 1)\n",
    "   */\n",
    "  def readCorpus(\n",
    "    file: String,\n",
    "    sampleSize: Double = 1.0)\n",
    "    (implicit sqlContext: SQLContext, sc: SparkContext): RDD[Document] = {\n",
    "\n",
    "    val log = LogManager.getLogger(CorpusReader.getClass)\n",
    "    var df: DataFrame = sqlContext.read.parquet(file)\n",
    "    df = df.where(df(\"type\").equalTo(\"ARTICLE\"))\n",
    "\n",
    "    val ioErrors: Accumulator[Int] = sc.accumulator(0, \"IO_ERRORS\")\n",
    "\n",
    "    // we might need to filter for only articles here but that wouldn't be a generelized solution.\n",
    "    val docs = (if(sampleSize == 1.0) df else df.sample(false, sampleSize)).flatMap{row =>\n",
    "      try {\n",
    "        val doc: Document = MemoryDocumentIO.getInstance().fromBytes(row.getAs(5): Array[Byte])\n",
    "        List(doc)\n",
    "      } catch {\n",
    "        case e:IOError =>\n",
    "          ioErrors.add(1)\n",
    "          List()\n",
    "      }\n",
    "    }\n",
    "    println(s\"$ioErrors IO Errors encountered\")\n",
    "    docs\n",
    "  }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# RelationsReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.SparkContext\n",
    "import org.apache.spark.rdd.RDD\n",
    "import org.apache.spark.sql.SQLContext\n",
    "\n",
    "case class EntityPair(source: String, dest: String)\n",
    "case class Relation(name: String, id: String, classIdx: Int, entities: Seq[EntityPair] = Seq())\n",
    "\n",
    "/** Reads relations data file into an RDD of [[Relation]]\n",
    " */\n",
    "object RelationsReader {\n",
    "  def readRelations(file: String)(implicit sqlContext: SQLContext): RDD[Relation] = {\n",
    "    import sqlContext.implicits._\n",
    "    sqlContext.read.parquet(file).as[Relation].rdd\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# FeatureExtractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.SparkContext\n",
    "import org.apache.spark.rdd.RDD\n",
    "import org.apache.spark.sql.SQLContext\n",
    "import se.lth.cs.docforia.Document\n",
    "import se.lth.cs.docforia.graph.disambig.NamedEntityDisambiguation\n",
    "import se.lth.cs.docforia.graph.text.{NamedEntity, Token}\n",
    "import se.lth.cs.docforia.query.QueryCollectors\n",
    "\n",
    "\n",
    "import scala.collection.JavaConverters._\n",
    "import scala.collection.mutable\n",
    "\n",
    "\n",
    "\n",
    "case class TrainingDataPoint(\n",
    "  relationId: String,\n",
    "  relationName: String,\n",
    "  relationClass: Long,\n",
    "  wordFeatures: Seq[String],\n",
    "  posFeatures: Seq[String],\n",
    "  ent1PosTags: Seq[String],\n",
    "  ent2PosTags: Seq[String],\n",
    "  ent1Type: String,\n",
    "  ent2Type: String)\n",
    "\n",
    "case class TestDataPoint(\n",
    "  sentence: Document,\n",
    "  qidSource: String,\n",
    "  qidDest: String,\n",
    "  wordFeatures: Seq[String],\n",
    "  posFeatures: Seq[String],\n",
    "  ent1PosFeatures: Seq[String],\n",
    "  ent2PosFeatures: Seq[String],\n",
    "  ent1Type: String,\n",
    "  ent2Type: String)\n",
    "\n",
    "case class FeatureArray(\n",
    "  sentence: Document,\n",
    "  subj: String,\n",
    "  obj: String,\n",
    "  wordFeatures: Seq[String],\n",
    "  posFeatures: Seq[String],\n",
    "  ent1PosFeatures: Seq[String],\n",
    "  ent2PosFeatures: Seq[String],\n",
    "  ent1Type: String,\n",
    "  ent2Type: String)\n",
    "\n",
    "case class TrainingSentence(\n",
    "  relationId: String,\n",
    "  relationName: String,\n",
    "  relationClass: Int,\n",
    "  sentenceDoc: Document,\n",
    "  entityPair: Seq[EntityPair])\n",
    "\n",
    "\n",
    "\n",
    "/** Extracts features for training/prediction\n",
    " */\n",
    "object FeatureExtractor {\n",
    "\n",
    "  val EMPTY_TOKEN = \"<empty>\"\n",
    "  val NBR_WORDS_BEFORE = 3\n",
    "  val NBR_WORDS_AFTER = 3\n",
    "  val MIN_FEATURE_LENGTH = 2\n",
    "\n",
    "  /** Returns an RDD of [[TrainingDataPoint]]\n",
    "    *\n",
    "    * Use this to collect training data for [[RelationModel]]\n",
    "    *\n",
    "    * @param trainingSentences  - an RDD of [[TrainingSentence]]\n",
    "    */\n",
    "  def trainingData(trainingSentences: RDD[TrainingSentence])(implicit sqlContext: SQLContext): RDD[TrainingDataPoint] = {\n",
    "\n",
    "    val trainingPoints = trainingSentences.flatMap(t => {\n",
    "      val neds = new mutable.HashSet() ++ t.entityPair.flatMap(p => Seq(p.source, p.dest))\n",
    "      val featureArrays = featureArray(t.sentenceDoc).flatMap(f => {\n",
    "        if (f.wordFeatures.length >= MIN_FEATURE_LENGTH) {\n",
    "          if (neds.contains(f.subj) && neds.contains(f.obj)) {\n",
    "            Seq(TrainingDataPoint(\n",
    "              t.relationId,\n",
    "              t.relationName,\n",
    "              t.relationClass,\n",
    "              f.wordFeatures,\n",
    "              f.posFeatures,\n",
    "              f.ent1PosFeatures,\n",
    "              f.ent2PosFeatures,\n",
    "              f.ent1Type,\n",
    "              f.ent2Type))\n",
    "          } else {\n",
    "            Seq(TrainingDataPoint(\"neg\", \"neg\", 0, f.wordFeatures, f.posFeatures, f.ent1PosFeatures, f.ent2PosFeatures,\n",
    "              f.ent1Type, f.ent2Type))\n",
    "          }\n",
    "        } else {\n",
    "          Seq()\n",
    "        }\n",
    "      })\n",
    "      featureArrays\n",
    "    })\n",
    "\n",
    "    trainingPoints.repartition(12)\n",
    "  }\n",
    "\n",
    "  /** Returns an RDD of [[TestDataPoint]]\n",
    "    *\n",
    "    *   Use this to collect test data for [[RelationModel]]\n",
    "    *\n",
    "    *   @param sentences  - a Seq of Docforia Documents\n",
    "    */\n",
    "  def testData(sentences: Seq[Document])(implicit sqlContext: SQLContext): Seq[TestDataPoint] = {\n",
    "\n",
    "    val testPoints = sentences.flatMap(sentence => {\n",
    "      featureArray(sentence).map(f => {\n",
    "        TestDataPoint(sentence, f.subj, f.obj, f.wordFeatures, f.posFeatures, f.ent1PosFeatures, f.ent2PosFeatures,\n",
    "          f.ent1Type, f.ent2Type)\n",
    "      })\n",
    "    })\n",
    "\n",
    "    testPoints\n",
    "  }\n",
    "\n",
    "  private def featureArray(sentence: Document): Seq[FeatureArray] = {\n",
    "\n",
    "    val NED = NamedEntityDisambiguation.`var`()\n",
    "    val NE = NamedEntity.`var`()\n",
    "    val T = Token.`var`()\n",
    "\n",
    "    sentence.nodes(classOf[Token])\n",
    "      .asScala\n",
    "      .toSeq\n",
    "      .zipWithIndex\n",
    "      .foreach(t => t._1.putTag(\"idx\", t._2))\n",
    "\n",
    "    val features = sentence.select(NED, T, NE)\n",
    "      .where(T)\n",
    "      .coveredBy(NED)\n",
    "      .where(T)\n",
    "      .coveredBy(NE)\n",
    "      .stream()\n",
    "      .collect(QueryCollectors.groupBy(sentence, NED, NE).values(T).collector())\n",
    "      .asScala\n",
    "      .toSet\n",
    "      .subsets(2)\n",
    "      .map(set => {\n",
    "        /*\n",
    "        Find the positions of the entities\n",
    "         */\n",
    "        val grp1 :: grp2 :: _ = set.toList\n",
    "\n",
    "        val start1 = grp1.value(0, T).getTag(\"idx\"): Int\n",
    "        val end1 = grp1.value(grp1.size() - 1, T).getTag(\"idx\"): Int\n",
    "\n",
    "        val start2 = grp2.value(0, T).getTag(\"idx\"): Int\n",
    "        val end2 = grp2.value(grp2.size() - 1, T).getTag(\"idx\"): Int\n",
    "\n",
    "        /* Windows of words and POS */\n",
    "        val words = tokenWindow(sentence, start1, end1, start2, end2, t => t.text)\n",
    "        val pos = tokenWindow(sentence, start1, end1, start2, end2, t => t.getPartOfSpeech)\n",
    "\n",
    "        /* Entity POS */\n",
    "        val ent1TokensPos = grp1.nodes[Token](T).asScala.toSeq.map(t => t.getPartOfSpeech).toArray\n",
    "        val ent2TokensPos = grp2.nodes[Token](T).asScala.toSeq.map(t => t.getPartOfSpeech).toArray\n",
    "\n",
    "        /* Entity Mention Type */\n",
    "        val ent1Type = if (grp1.key(NE).hasLabel) grp1.key(NE).getLabel else \"<missing label>\"\n",
    "        val ent2Type = if (grp2.key(NE).hasLabel) grp2.key(NE).getLabel else \"<missing label>\"\n",
    "\n",
    "        FeatureArray(\n",
    "          sentence,\n",
    "          grp1.key(NED).getIdentifier.split(\":\").last,\n",
    "          grp2.key(NED).getIdentifier.split(\":\").last,\n",
    "          words,\n",
    "          pos,\n",
    "          ent1TokensPos,\n",
    "          ent2TokensPos,\n",
    "          ent1Type,\n",
    "          ent2Type)\n",
    "      }).toSeq\n",
    "\n",
    "    features\n",
    "\n",
    "  }\n",
    "\n",
    "  /** Extract string features from a Token window around two entities.\n",
    "    */\n",
    "  private def tokenWindow(sentence: Document, start1: Int, end1: Int, start2: Int, end2: Int, f: Token => String): Seq[String] = {\n",
    "    /*\n",
    "      Extract words before and after entity 1\n",
    "     */\n",
    "    val wordsBefore1 = sentence.nodes(classOf[Token]).asScala.toSeq.slice(start1 - NBR_WORDS_BEFORE, start1)\n",
    "    val wordsAfter1 = sentence.nodes(classOf[Token]).asScala.toSeq.slice(end1 + NBR_WORDS_AFTER, end1 + NBR_WORDS_AFTER + 1)\n",
    "\n",
    "    /*\n",
    "      Extract words before and after entity 2\n",
    "     */\n",
    "    val wordsBefore2 = sentence.nodes(classOf[Token]).asScala.toSeq.slice(start2 - NBR_WORDS_BEFORE, start2)\n",
    "    val wordsAfter2 = sentence.nodes(classOf[Token]).asScala.toSeq.slice(end2 + NBR_WORDS_AFTER, end2 + NBR_WORDS_AFTER + 1)\n",
    "\n",
    "    /*\n",
    "      Create string feature vector for the pair\n",
    "     */\n",
    "    val features = Seq(\n",
    "      Seq.fill(NBR_WORDS_BEFORE - wordsBefore1.length)(EMPTY_TOKEN) ++ wordsBefore1.map(f),\n",
    "      wordsAfter1.map(f) ++ Seq.fill(NBR_WORDS_AFTER - wordsAfter1.length)(EMPTY_TOKEN),\n",
    "      Seq.fill(NBR_WORDS_BEFORE - wordsBefore2.length)(EMPTY_TOKEN) ++ wordsBefore2.map(f),\n",
    "      wordsAfter2.map(f) ++ Seq.fill(NBR_WORDS_AFTER - wordsAfter2.length)(EMPTY_TOKEN)\n",
    "    ).flatten\n",
    "    features\n",
    "  }\n",
    "\n",
    "  /** Saves the training data to the path\n",
    "   */\n",
    "  def save(data: RDD[TrainingDataPoint], path: String)(implicit sqlContext: SQLContext): Unit = {\n",
    "    import sqlContext.implicits._\n",
    "    data.toDF().write.json(path)\n",
    "  }\n",
    "\n",
    "  /** Loads the data from path\n",
    "   */\n",
    "  def load(path: String)(implicit sqlContext: SQLContext): RDD[TrainingDataPoint]  = {\n",
    "    import sqlContext.implicits._\n",
    "    sqlContext.read.json(path).as[TrainingDataPoint].rdd\n",
    "  }\n",
    "\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# TrainingDataExtractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import org.apache.log4j.LogManager\n",
    "import org.apache.spark.SparkContext\n",
    "import org.apache.spark.rdd.RDD\n",
    "import org.apache.spark.sql.SQLContext\n",
    "import se.lth.cs.docforia.Document\n",
    "import se.lth.cs.docforia.graph.disambig.NamedEntityDisambiguation\n",
    "import se.lth.cs.docforia.graph.text.Sentence\n",
    "import se.lth.cs.docforia.memstore.MemoryDocumentIO\n",
    "import se.lth.cs.docforia.query.QueryCollectors\n",
    "\n",
    "\n",
    "import scala.collection.JavaConverters._\n",
    "import scala.collection.mutable\n",
    "import scala.collection.mutable.ListBuffer\n",
    "\n",
    "\n",
    "case class TrainingSentence(\n",
    "  relationId: String,\n",
    "  relationName: String,\n",
    "  relationClass: Int,\n",
    "  sentenceDoc: Document,\n",
    "  entityPair: Seq[EntityPair])\n",
    "\n",
    "private case class SerializedTrainingSentence(\n",
    "  relationId: String,\n",
    "  relationName: String,\n",
    "  relationClass: Int,\n",
    "  sentenceDoc: Array[Byte],\n",
    "  entityPair: Seq[EntityPair])\n",
    "\n",
    "\n",
    "/** Provides training data extraction\n",
    " */\n",
    "object TrainingDataExtractor {\n",
    "\n",
    "  val SENTENCE_MAX_LENGTH = 220\n",
    "  val SENTENCE_MIN_LENGTH = 5\n",
    "\n",
    "  def printInfo(docs: RDD[Document], relations: RDD[Relation], sentences: RDD[TrainingSentence]): Unit = {\n",
    "    val log = LogManager.getLogger(TrainingDataExtractor.getClass)\n",
    "    val docsCount = docs.count()\n",
    "    val relationsCount = relations.count()\n",
    "    val entitiesCount = relations.map(r => (r.id -> r.entities.length)).collect().toMap\n",
    "    //val dist = sentences.map(t => (t.relationId, 1)).reduceByKey(_ + _)\n",
    "    //  .map(t => s\"${t._1}\\t-> ${entitiesCount.getOrElse(t._1, 0)} entities\\t-> ${t._2}\").collect()\n",
    "\n",
    "    log.info(\"Extracting Training Sentences\")\n",
    "    log.info(s\"Documents: $docsCount\")\n",
    "    log.info(s\"Relations: $relationsCount\")\n",
    "    //dist.map(log.info)\n",
    "  }\n",
    "\n",
    "  /**\n",
    "   * Extracts RDD of [[TrainingSentence]]\n",
    "   */\n",
    "  def extract(docs: RDD[Document], relations: RDD[Relation])(implicit sparkContext: SparkContext): RDD[TrainingSentence] = {\n",
    "\n",
    "    val relMapping = relations.map(r => {\n",
    "      // val entPairMap = new Object2ObjectOpenHashMap[String, mutable.Set[String]]()\n",
    "      val entPairMap = scala.collection.mutable.Map[String, mutable.Set[String]]()\n",
    "        r.entities.foreach(ep =>{\n",
    "          val dests = entPairMap.getOrElse(ep.source, new mutable.HashSet[String]())\n",
    "          dests += ep.dest\n",
    "          entPairMap.put(ep.source, dests)\n",
    "        })\n",
    "      (r, entPairMap)\n",
    "    })\n",
    "\n",
    "    val broadcastRM = sparkContext.broadcast(relMapping.collect())\n",
    "\n",
    "    val data:RDD[TrainingSentence] = docs.flatMap(doc => {\n",
    "\n",
    "        val S = Sentence.`var`()\n",
    "        val NED = NamedEntityDisambiguation.`var`()\n",
    "\n",
    "        val trainingSentences:Seq[TrainingSentence] = doc.select(S, NED)\n",
    "          .where(NED)\n",
    "          .coveredBy(S)\n",
    "          .stream()\n",
    "          .collect(QueryCollectors.groupBy(doc, S).values(NED).collector()).asScala\n",
    "          .filter(pg => SENTENCE_MIN_LENGTH <= pg.key(S).length() && pg.key(S).length() <= SENTENCE_MAX_LENGTH)\n",
    "          .flatMap(pg => {\n",
    "\n",
    "            val neds = pg.list(NED).asScala.map(_.getIdentifier.split(\":\").last).toSet.subsets(2).map(_.toSeq).toSeq\n",
    "            lazy val sDoc = doc.subDocument(pg.key(S).getStart, pg.key(S).getEnd)\n",
    "\n",
    "            val trainingData = broadcastRM.value.flatMap{\n",
    "              case (relation, mapping) => {\n",
    "                val pairs = neds.flatMap(pair => {\n",
    "                  val foundPairs = ListBuffer[EntityPair]()\n",
    "                  if (mapping.getOrElse(pair(0), mutable.Set.empty).contains(pair(1))){\n",
    "                    foundPairs += EntityPair(pair(0), pair(1))\n",
    "                  } else if (mapping.getOrElse(pair(1), mutable.Set.empty).contains(pair(0))){\n",
    "                    foundPairs += EntityPair(pair(1), pair(0))\n",
    "                  }\n",
    "                  foundPairs\n",
    "                })\n",
    "\n",
    "                if (pairs.length > 0)\n",
    "                  Seq(TrainingSentence(relation.id, relation.name, relation.classIdx, sDoc, pairs.toList))\n",
    "                else\n",
    "                  Seq()\n",
    "              }\n",
    "            }\n",
    "\n",
    "            trainingData\n",
    "          })\n",
    "\n",
    "        trainingSentences\n",
    "      })\n",
    "\n",
    "    data.repartition(12)\n",
    "  }\n",
    "\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 IO Errors encountered\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Name: org.apache.spark.SparkException\n",
       "Message: Task not serializable\n",
       "StackTrace: org.apache.spark.util.ClosureCleaner$.ensureSerializable(ClosureCleaner.scala:304)\n",
       "org.apache.spark.util.ClosureCleaner$.org$apache$spark$util$ClosureCleaner$$clean(ClosureCleaner.scala:294)\n",
       "org.apache.spark.util.ClosureCleaner$.clean(ClosureCleaner.scala:122)\n",
       "org.apache.spark.SparkContext.clean(SparkContext.scala:2055)\n",
       "org.apache.spark.rdd.RDD$$anonfun$flatMap$1.apply(RDD.scala:333)\n",
       "org.apache.spark.rdd.RDD$$anonfun$flatMap$1.apply(RDD.scala:332)\n",
       "org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150)\n",
       "org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:111)\n",
       "org.apache.spark.rdd.RDD.withScope(RDD.scala:316)\n",
       "org.apache.spark.rdd.RDD.flatMap(RDD.scala:332)\n",
       "$line276.$read$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$TrainingDataExtractor$.extract(<console>:277)\n",
       "$line282.$read$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:238)\n",
       "$line282.$read$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:243)\n",
       "$line282.$read$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:245)\n",
       "$line282.$read$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:247)\n",
       "$line282.$read$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:249)\n",
       "$line282.$read$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:251)\n",
       "$line282.$read$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:253)\n",
       "$line282.$read$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:255)\n",
       "$line282.$read$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:257)\n",
       "$line282.$read$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:259)\n",
       "$line282.$read$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:261)\n",
       "$line282.$read$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:263)\n",
       "$line282.$read$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:265)\n",
       "$line282.$read$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:267)\n",
       "$line282.$read$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:269)\n",
       "$line282.$read$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:271)\n",
       "$line282.$read$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:273)\n",
       "$line282.$read$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:275)\n",
       "$line282.$read$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:277)\n",
       "$line282.$read$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:279)\n",
       "$line282.$read$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:281)\n",
       "$line282.$read$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:283)\n",
       "$line282.$read$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:285)\n",
       "$line282.$read$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:287)\n",
       "$line282.$read$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:289)\n",
       "$line282.$read$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:291)\n",
       "$line282.$read$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:293)\n",
       "$line282.$read$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:295)\n",
       "$line282.$read$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:297)\n",
       "$line282.$read$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:299)\n",
       "$line282.$read$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:301)\n",
       "$line282.$read$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:303)\n",
       "$line282.$read$$iwC$$iwC$$iwC$$iwC.<init>(<console>:305)\n",
       "$line282.$read$$iwC$$iwC$$iwC.<init>(<console>:307)\n",
       "$line282.$read$$iwC$$iwC.<init>(<console>:309)\n",
       "$line282.$read$$iwC.<init>(<console>:311)\n",
       "$line282.$read.<init>(<console>:313)\n",
       "$line282.$read$.<init>(<console>:317)\n",
       "$line282.$read$.<clinit>(<console>)\n",
       "$line282.$eval$.<init>(<console>:7)\n",
       "$line282.$eval$.<clinit>(<console>)\n",
       "$line282.$eval.$print(<console>)\n",
       "sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
       "sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
       "sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
       "java.lang.reflect.Method.invoke(Method.java:497)\n",
       "org.apache.spark.repl.SparkIMain$ReadEvalPrint.call(SparkIMain.scala:1065)\n",
       "org.apache.spark.repl.SparkIMain$Request.loadAndRun(SparkIMain.scala:1346)\n",
       "org.apache.spark.repl.SparkIMain.loadAndRunReq$1(SparkIMain.scala:840)\n",
       "org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:871)\n",
       "org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:819)\n",
       "org.apache.toree.kernel.interpreter.scala.ScalaInterpreter$$anonfun$interpretAddTask$1$$anonfun$apply$3.apply(ScalaInterpreter.scala:361)\n",
       "org.apache.toree.kernel.interpreter.scala.ScalaInterpreter$$anonfun$interpretAddTask$1$$anonfun$apply$3.apply(ScalaInterpreter.scala:356)\n",
       "org.apache.toree.global.StreamState$.withStreams(StreamState.scala:81)\n",
       "org.apache.toree.kernel.interpreter.scala.ScalaInterpreter$$anonfun$interpretAddTask$1.apply(ScalaInterpreter.scala:355)\n",
       "org.apache.toree.kernel.interpreter.scala.ScalaInterpreter$$anonfun$interpretAddTask$1.apply(ScalaInterpreter.scala:355)\n",
       "org.apache.toree.utils.TaskManager$$anonfun$add$2$$anon$1.run(TaskManager.scala:140)\n",
       "java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n",
       "java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n",
       "java.lang.Thread.run(Thread.java:745)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val docs = CorpusReader.readCorpus(\"/Users/axel/utveckling/prometheus/data/wikipedia-corpus-herd/sv\")(sqlContext, sc)\n",
    "val relations = RelationsReader.readRelations(\"/Users/axel/utveckling/prometheus/data/0.4.0/entities\")(sqlContext)\n",
    "val sentences = TrainingDataExtractor.extract(docs, relations)(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: Compile Error\n",
       "Message: <console>:19: error: not found: type Document\n",
       "         sentenceDoc: Document,\n",
       "                      ^\n",
       "<console>:20: error: not found: type EntityPair\n",
       "         entityPair: Seq[EntityPair])\n",
       "                         ^\n",
       "StackTrace: "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "case class TrainingSentence(\n",
    "  relationId: String,\n",
    "  relationName: String,\n",
    "  relationClass: Int,\n",
    "  sentenceDoc: Document,\n",
    "  entityPair: Seq[EntityPair])\n",
    "\n",
    "private case class SerializedTrainingSentence(\n",
    "  relationId: String,\n",
    "  relationName: String,\n",
    "  relationClass: Int,\n",
    "  sentenceDoc: Array[Byte],\n",
    "  entityPair: Seq[EntityPair])\n",
    "\n",
    "val trainingSentences = sqlContext.read.parquet(\"/Users/axel/utveckling/prometheus/data/0.1.0-32-g4d0b685/relation_model/en/training_sentences\")\n",
    "rawData.map(st => {\n",
    "  TrainingSentence(st.relationId, st.relationName, st.relationClass,\n",
    "                   MemoryDocumentIO.getInstance().fromBytes(st.sentenceDoc), st.entityPair)\n",
    "})\n",
    "\n",
    "\n",
    "trainingSentences.printSchema()\n",
    "trainingSentences.take(1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "name": "scala",
   "version": "2.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
